{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference = 'https://mccormickml.com/2019/07/22/BERT-fine-tuning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "WARNING:tensorflow:From <ipython-input-2-498147d11be9>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "#gpu 확인\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(device_name)\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화리뷰 감정분석 데이터 다운로드\n",
    "!git clone https://github.com/e9t/nsmc.git\n",
    "    \n",
    "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t') #훈련\n",
    "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t') #테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측치제거\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "#중복제거\n",
    "train.drop_duplicates(subset='document', inplace=True)\n",
    "test.drop_duplicates(subset='document', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert tokenizer에 맞게 형식 변환 - CLS = classification / SEP = seperation\n",
    "train['document'] = train['document'].apply(lambda x: \"[CLS] \" + str(x) + \" [SEP]\")\n",
    "test['document'] = test['document'].apply(lambda x: \"[CLS] \" + str(x) + \" [SEP]\")\n",
    "\n",
    "'''\n",
    "BERT tutorial에선\n",
    "모든 sentence의 시작엔 CLS를, 끝엔 SEP를 붙여야 한다고 이야기하고 있다. \n",
    "이때 sentence의 단위를 문장의 단위로 보아야할까, 하나의 리뷰 단위로 보아야할까?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train['document']\n",
    "labels = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBERT는 형태소분석을 통해 토큰을 분리하지 않는다.\\nWordPiece를 사용, 한 단어 내에서 자주 나오는 글자들을 붙여 하나의 토큰으로 생성하기에 언어에 상관없이 토큰 생성 가능\\n신조어 같이 사전에 없는 단어를 처리하기에도 좋다.\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "tokenized_texts[0]\n",
    "\n",
    "'''\n",
    "BERT는 형태소분석을 통해 토큰을 분리하지 않는다.\n",
    "WordPiece를 사용, 한 단어 내에서 자주 나오는 글자들을 붙여 하나의 토큰으로 생성하기에 언어에 상관없이 토큰 생성 가능\n",
    "신조어 같이 사전에 없는 단어를 처리하기에도 좋다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이: 142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bc4f5513c8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#토큰 문장 최대 길이 확인\n",
    "maxlen = max(len(I) for I in tokenized_texts) \n",
    "print('문장의 최대 길이: {}'.format(maxlen))\n",
    "\n",
    "count_len = pd.Series(tokenized_texts).apply(len)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "sns.distplot(count_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT의 제약조건  \n",
    "- 모든 문장은 padded나 truncated 되어야 하며, 단일하고 고정된 길이를 가져야함\n",
    "- 최대 길이는 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n",
       "         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n",
       "          102,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 최대 길이 지정 - 분포에 따라 최대길이 조정하며 결과값을 비교하자\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰 정수 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 최대 길이에 맞춰 자르고 패딩 추가 - 패딩도 바꿀수 있지 않나? \n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 토큰이라면 1, 패딩이라면 0으로 설정 - 패딩은 어텐션 수행 X -> 속도향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "\n",
    "# # For every sentence...\n",
    "# for sent in sentences:\n",
    "#     # `encode_plus` will:\n",
    "#     #   (1) Tokenize the sentence.\n",
    "#     #   (2) Prepend the `[CLS]` token to the start.\n",
    "#     #   (3) Append the `[SEP]` token to the end.\n",
    "#     #   (4) Map tokens to their IDs.\n",
    "#     #   (5) Pad or truncate the sentence to `max_length`\n",
    "#     #   (6) Create attention masks for [PAD] tokens.\n",
    "#     encoded_dict = tokenizer.encode_plus(\n",
    "#                         sent,                      # Sentence to encode.\n",
    "#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                         max_length = 64,           # Pad & truncate all sentences.\n",
    "#                         pad_to_max_length = True,\n",
    "#                         return_attention_mask = True,   # Construct attn. masks.\n",
    "#                         return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                    )\n",
    "    \n",
    "#     # Add the encoded sentence to the list.    \n",
    "#     input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "#     # And its attention mask (simply differentiates padding from non-padding).\n",
    "#     attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# # Convert the lists into tensors.\n",
    "# input_ids = torch.cat(input_ids, dim=0)\n",
    "# attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(labels)\n",
    "\n",
    "# # Print sentence 0, now as a list of IDs.\n",
    "# print('Original: ', sentences[0])\n",
    "# print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2018, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2018, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈 - 16, 32를 추천하고 있다.\n",
    "batch_size = 32\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴, 즉 지금은 32개의 텐서씩 학습 - GPU 메모리와 관련\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data) #데이터 셔플을 위해 RandomSampler 사용 - 데이터로더는 index로 파일을 불러온다.\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = test['document']\n",
    "\n",
    "labels = test['label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 굳 ㅋ [SEP]\n",
      "['[CLS]', '굳', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (sentences[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)\n",
      "tensor(1)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 파이토치의 텐서로 변환\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 32\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "튜토리얼에서 fine-tuning을 위해 추천하는 parameter\n",
    "\n",
    "Batch size: 16, 32\n",
    "Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류를 위한 BERT 모델 생성\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2) #num_labels = 이진분류기에 2\n",
    "model.cuda() #gpu 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4 #통상적으로 EPOCHS는 2-4 사이, 그 이상은 overfit의 위험이 존재\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 학습률을 조금씩 감소시키는 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        \n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "dㅇ\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "# gradient는 역전파 단계에서 자동으로 축적이 된다.\n",
    "\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운 문장 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    b_input_ids = torch.tensor(b_input_ids).to(device).long()\n",
    "        \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving model to ./model_save/\n",
    "\n",
    "('./model_save/vocab.txt',\n",
    " './model_save/special_tokens_map.json',\n",
    " './model_save/added_tokens.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = model_class.from_pretrained(output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
