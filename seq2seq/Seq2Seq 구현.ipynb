{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- https://wikidocs.net/24996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/spa-eng.zip'\n",
    "filename = 'spa-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125446"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines= pd.read_csv('spa.txt', names=['src', 'tar', 'what'], sep='\\t') \n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68043</th>\n",
       "      <td>Few people know about the plan.</td>\n",
       "      <td>Pocas personas saben del plan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34052</th>\n",
       "      <td>I run five miles a day.</td>\n",
       "      <td>Corro cinco millas al día.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>I need cash.</td>\n",
       "      <td>Necesito dinero en efectivo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27622</th>\n",
       "      <td>We don't do anything.</td>\n",
       "      <td>No hacemos nada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76967</th>\n",
       "      <td>I have a lot of books in my room.</td>\n",
       "      <td>Tengo muchos libros en mi habitación.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33276</th>\n",
       "      <td>His reply was negative.</td>\n",
       "      <td>Su respuesta fue negativa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11155</th>\n",
       "      <td>I can't stop you.</td>\n",
       "      <td>No te puedo parar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>We're home.</td>\n",
       "      <td>Estamos en casa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9169</th>\n",
       "      <td>Keep practicing.</td>\n",
       "      <td>Sigue practicando.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78355</th>\n",
       "      <td>The cat is sleeping on the chair.</td>\n",
       "      <td>El gato duerme sobre la silla.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     src  \\\n",
       "68043    Few people know about the plan.   \n",
       "34052            I run five miles a day.   \n",
       "2017                        I need cash.   \n",
       "27622              We don't do anything.   \n",
       "76967  I have a lot of books in my room.   \n",
       "33276            His reply was negative.   \n",
       "11155                  I can't stop you.   \n",
       "1684                         We're home.   \n",
       "9169                    Keep practicing.   \n",
       "78355  The cat is sleeping on the chair.   \n",
       "\n",
       "                                         tar  \n",
       "68043         Pocas personas saben del plan.  \n",
       "34052             Corro cinco millas al día.  \n",
       "2017            Necesito dinero en efectivo.  \n",
       "27622                       No hacemos nada.  \n",
       "76967  Tengo muchos libros en mi habitación.  \n",
       "33276             Su respuesta fue negativa.  \n",
       "11155                     No te puedo parar.  \n",
       "1684                        Estamos en casa.  \n",
       "9169                      Sigue practicando.  \n",
       "78355         El gato duerme sobre la silla.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:100000] # 10만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24638</th>\n",
       "      <td>He's eating an apple.</td>\n",
       "      <td>\\t Se está comiendo una manzana. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10484</th>\n",
       "      <td>Did Tom say that?</td>\n",
       "      <td>\\t ¿Eso dijo Tom? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31672</th>\n",
       "      <td>What's my room number?</td>\n",
       "      <td>\\t ¿Cuál es el número de mi habitación? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>Mow the lawn.</td>\n",
       "      <td>\\t Corta el césped. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54229</th>\n",
       "      <td>Did you watch TV last night?</td>\n",
       "      <td>\\t ¿Viste la televisión la pasada noche? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39021</th>\n",
       "      <td>Please buy me this book.</td>\n",
       "      <td>\\t Cómprame este libro, por favor. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48809</th>\n",
       "      <td>Tom is acting like a baby.</td>\n",
       "      <td>\\t Tom está actuando como un bebe. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51162</th>\n",
       "      <td>I play a lot of volleyball.</td>\n",
       "      <td>\\t Juego mucho al voleibol. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>Mama cried.</td>\n",
       "      <td>\\t Mamá lloró. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40927</th>\n",
       "      <td>Your lipstick's smeared.</td>\n",
       "      <td>\\t Tu barra de labios se ha corrido. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                src  \\\n",
       "24638         He's eating an apple.   \n",
       "10484             Did Tom say that?   \n",
       "31672        What's my room number?   \n",
       "3366                  Mow the lawn.   \n",
       "54229  Did you watch TV last night?   \n",
       "39021      Please buy me this book.   \n",
       "48809    Tom is acting like a baby.   \n",
       "51162   I play a lot of volleyball.   \n",
       "1428                    Mama cried.   \n",
       "40927      Your lipstick's smeared.   \n",
       "\n",
       "                                               tar  \n",
       "24638          \\t Se está comiendo una manzana. \\n  \n",
       "10484                         \\t ¿Eso dijo Tom? \\n  \n",
       "31672   \\t ¿Cuál es el número de mi habitación? \\n  \n",
       "3366                        \\t Corta el césped. \\n  \n",
       "54229  \\t ¿Viste la televisión la pasada noche? \\n  \n",
       "39021        \\t Cómprame este libro, por favor. \\n  \n",
       "48809        \\t Tom está actuando como un bebe. \\n  \n",
       "51162               \\t Juego mucho al voleibol. \\n  \n",
       "1428                             \\t Mamá lloró. \\n  \n",
       "40927      \\t Tu barra de labios se ha corrido. \\n  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 시작과 끝을 나타내는 토큰 설정\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 집합 구축\n",
    "src_vocab=set()\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "#training set에서 나타난 알파벳들을 저장\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '\\xa0': 76, '°': 77, 'á': 78, 'ã': 79, 'è': 80, 'é': 81, 'ö': 82, '‘': 83, '’': 84, '₂': 85, '€': 86}\n",
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '+': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '¡': 79, '«': 80, '°': 81, 'º': 82, '»': 83, '¿': 84, 'Á': 85, 'É': 86, 'Ó': 87, 'Ú': 88, 'á': 89, 'è': 90, 'é': 91, 'í': 92, 'ñ': 93, 'ó': 94, 'ö': 95, 'ú': 96, 'ü': 97, 'ś': 98, 'с': 99, '\\u200b': 100, '—': 101, '€': 102}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)\n",
    "#각 알파벳들에 index 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩\n",
    "- 입력 데이터와 출력 데이터 모두를 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 9], [30, 64, 9], [30, 64, 9], [30, 64, 9], [31, 58, 9]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 57, 12], [48, 57, 72, 57, 12], [48, 53, 77, 53, 12], [48, 89, 77, 53, 71, 57, 12], [34, 67, 64, 53, 12]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57, 12], [57, 72, 57, 12], [53, 77, 53, 12], [89, 77, 53, 71, 57, 12], [67, 64, 53, 12]]\n"
     ]
    }
   ],
   "source": [
    "#실제 값에서는 \\t와 \\n이 필요하지 않다.\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어의 최대길이: 41\n",
      "스페인어의 최대길이: 92\n"
     ]
    }
   ],
   "source": [
    "#패딩값을 주기 위해 최대 길이를 조절\n",
    "#영어의 최대 길이와 스페인어의 최대 길이를 맞춰줄 필요는 없다. 영어는 영어끼리, 스페인어는 스페인어끼리\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print('영어의 최대길이:', max_src_len)\n",
    "print('스페인어의 최대길이:', max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 값에 대하여 one_hot 인코딩을 수행\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "#인코더 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True/ 인코더에 입력을 넣으면 내부 상태를 리턴\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태(h)와 셀(c) 상태. 은닉상태와 셀 상태 두가지를 전달\n",
    "#encoder_state에 은닉상태와 셀 상태 두가지를 저장하고, 이를 디코더에 전달하여 두 가지 상태 모두를 디코더로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 마지막 은닉 상태, 셀 상태로. 이에 따라 initial_state를 encoder_state로\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[functional_1/lstm/PartitionedCall]] [Op:__inference_train_function_4861]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d40ee9abd90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[functional_1/lstm/PartitionedCall]] [Op:__inference_train_function_4861]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=128, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Váyase. \n",
      "번역기가 번역한 문장: nallo me esta cantado. \n",
      "-----------------------------------\n",
      "입력 문장: Attack!\n",
      "정답 문장:  ¡Ataque! \n",
      "번역기가 번역한 문장: meó me de cantar a Tom. \n",
      "-----------------------------------\n",
      "입력 문장: Really?\n",
      "정답 문장:  ¿En serio? \n",
      "번역기가 번역한 문장: me e una de cana de cama. \n",
      "-----------------------------------\n",
      "입력 문장: Get lost!\n",
      "정답 문장:  ¡Piérdete! \n",
      "번역기가 번역한 문장: ómó muidado a Tom. \n",
      "-----------------------------------\n",
      "입력 문장: Ask anyone.\n",
      "정답 문장:  Pregúntele a cualquiera. \n",
      "번역기가 번역한 문장: meó me de allo. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'spa-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "#샘플 사용량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 위에서 구현한 함수를 내부적으로 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "b'avez vous deja dine ?'\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "교사 강요 (Teacher Forcing)을 위해 훈련에 사용할 디코더의 입력 시퀀스와\n",
    "실제 값에 해당하는 출력 시퀀스를 따로 분리하여 저장.\n",
    "입력 시퀀스에는 <SOS>를, 출력 시퀀스에는 <EOS>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"spa.txt\", \"r\", encoding='UTF8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line_input)\n",
    "            decoder_input.append(tar_line_input)\n",
    "            decoder_target.append(tar_line_target)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "[['<sos>', 've', '.'], ['<sos>', 'vete', '.'], ['<sos>', 'vaya', '.'], ['<sos>', 'vayase', '.'], ['<sos>', 'hola', '.']]\n",
      "[['ve', '.', '<eos>'], ['vete', '.', '<eos>'], ['vaya', '.', '<eos>'], ['vayase', '.', '<eos>'], ['hola', '.', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_spa_in, sents_spa_out = load_preprocessed_data()\n",
    "print(sents_en_in[:5])\n",
    "print(sents_spa_in[:5])\n",
    "print(sents_spa_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<sos>', 've', '.'],\n",
       " ['<sos>', 'vete', '.'],\n",
       " ['<sos>', 'vaya', '.'],\n",
       " ['<sos>', 'vayase', '.'],\n",
       " ['<sos>', 'hola', '.'],\n",
       " ['<sos>', 'corre', '!'],\n",
       " ['<sos>', 'corran', '!'],\n",
       " ['<sos>', 'corra', '!'],\n",
       " ['<sos>', 'corred', '!'],\n",
       " ['<sos>', 'corred', '.'],\n",
       " ['<sos>', 'quien', '?'],\n",
       " ['<sos>', 'orale', '!'],\n",
       " ['<sos>', 'fuego', '!'],\n",
       " ['<sos>', 'incendio', '!'],\n",
       " ['<sos>', 'disparad', '!'],\n",
       " ['<sos>', 'ayuda', '!'],\n",
       " ['<sos>', 'socorro', '!', 'auxilio', '!'],\n",
       " ['<sos>', 'auxilio', '!'],\n",
       " ['<sos>', 'salta', '!'],\n",
       " ['<sos>', 'salte', '.'],\n",
       " ['<sos>', 'parad', '!'],\n",
       " ['<sos>', 'para', '!'],\n",
       " ['<sos>', 'pare', '!'],\n",
       " ['<sos>', 'espera', '!'],\n",
       " ['<sos>', 'esperen', '.'],\n",
       " ['<sos>', 'continua', '.'],\n",
       " ['<sos>', 'continue', '.'],\n",
       " ['<sos>', 'hola', '.'],\n",
       " ['<sos>', 'date', 'prisa', '!'],\n",
       " ['<sos>', 'daos', 'prisa', '!'],\n",
       " ['<sos>', 'dese', 'prisa', '.'],\n",
       " ['<sos>', 'me', 'oculte', '.'],\n",
       " ['<sos>', 'me', 'escondi', '.'],\n",
       " ['<sos>', 'me', 'ocultaba', '.'],\n",
       " ['<sos>', 'me', 'escondia', '.'],\n",
       " ['<sos>', 'corri', '.'],\n",
       " ['<sos>', 'corria', '.'],\n",
       " ['<sos>', 'lo', 'intento', '.'],\n",
       " ['<sos>', 'he', 'ganado', '!'],\n",
       " ['<sos>', 'oh', 'no', '!'],\n",
       " ['<sos>', 'tomatelo', 'con', 'soda', '.'],\n",
       " ['<sos>', 'fuego', '!'],\n",
       " ['<sos>', 'disparad', '!'],\n",
       " ['<sos>', 'disparen', '!'],\n",
       " ['<sos>', 'dispara', '!'],\n",
       " ['<sos>', 'dispara', '!'],\n",
       " ['<sos>', 'dispare', '!'],\n",
       " ['<sos>', 'sonrie', '.'],\n",
       " ['<sos>', 'al', 'ataque', '!'],\n",
       " ['<sos>', 'atacad', '!'],\n",
       " ['<sos>', 'ataque', '!'],\n",
       " ['<sos>', 'ataquen', '!'],\n",
       " ['<sos>', 'ataca', '!'],\n",
       " ['<sos>', 'levanta', '.'],\n",
       " ['<sos>', 've', 'ahora', 'mismo', '.'],\n",
       " ['<sos>', 'id', 'ahora', 'mismo', '.'],\n",
       " ['<sos>', 'vaya', 'ahora', 'mismo', '.'],\n",
       " ['<sos>', 'vayan', 'ahora', 'mismo', '.'],\n",
       " ['<sos>', 've', 'ya', '.'],\n",
       " ['<sos>', 'id', 'ya', '.'],\n",
       " ['<sos>', 'vaya', 'ya', '.'],\n",
       " ['<sos>', 'vayan', 'ya', '.'],\n",
       " ['<sos>', 'lo', 'tengo', '!'],\n",
       " ['<sos>', 'lo', 'pillas', '?'],\n",
       " ['<sos>', 'entendiste', '?'],\n",
       " ['<sos>', 'el', 'corrio', '.'],\n",
       " ['<sos>', 'metete', 'adentro', '.'],\n",
       " ['<sos>', 'abrazame', '.'],\n",
       " ['<sos>', 'me', 'preocupo', '.'],\n",
       " ['<sos>', 'me', 'cai', '.'],\n",
       " ['<sos>', 'hui', '.'],\n",
       " ['<sos>', 'me', 'escape', '.'],\n",
       " ['<sos>', 'huia', '.'],\n",
       " ['<sos>', 'me', 'escapaba', '.'],\n",
       " ['<sos>', 'yo', 'lo', 'se', '.'],\n",
       " ['<sos>', 'sali', '.'],\n",
       " ['<sos>', 'menti', '.'],\n",
       " ['<sos>', 'perdi', '.'],\n",
       " ['<sos>', 'dimito', '.'],\n",
       " ['<sos>', 'renuncie', '.'],\n",
       " ['<sos>', 'lo', 'dejo', '.'],\n",
       " ['<sos>', 'cante', '.'],\n",
       " ['<sos>', 'llore', '.'],\n",
       " ['<sos>', 'lloraba', '.'],\n",
       " ['<sos>', 'estoy', 'trabajando', '.'],\n",
       " ['<sos>', 'tengo', 'diecinueve', '.'],\n",
       " ['<sos>', 'estoy', 'levantado', '.'],\n",
       " ['<sos>', 'escucha', '.'],\n",
       " ['<sos>', 'escuche', '.'],\n",
       " ['<sos>', 'escuchen', '.'],\n",
       " ['<sos>', 'no', 'puede', 'ser', '!'],\n",
       " ['<sos>', 'de', 'ninguna', 'manera', '.'],\n",
       " ['<sos>', 'de', 'ninguna', 'manera', '!'],\n",
       " ['<sos>', 'imposible', '!'],\n",
       " ['<sos>', 'de', 'ningun', 'modo', '!'],\n",
       " ['<sos>', 'de', 'eso', 'nada', '!'],\n",
       " ['<sos>', 'ni', 'cagando', '!'],\n",
       " ['<sos>', 'mangos', '!'],\n",
       " ['<sos>', 'minga', '!'],\n",
       " ['<sos>', 'ni', 'en', 'pedo', '!']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_spa_in[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "\n",
    "tokenizer_spa = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_spa.fit_on_texts(sents_spa_in)\n",
    "tokenizer_spa.fit_on_texts(sents_spa_out)\n",
    "decoder_input = tokenizer_spa.texts_to_sequences(sents_spa_in)\n",
    "decoder_target = tokenizer_spa.texts_to_sequences(sents_spa_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 302, 1],\n",
       " [2, 1052, 1],\n",
       " [2, 479, 1],\n",
       " [2, 3932, 1],\n",
       " [2, 1504, 1],\n",
       " [2, 1317, 61],\n",
       " [2, 6014, 61],\n",
       " [2, 7128, 61],\n",
       " [2, 6015, 61],\n",
       " [2, 6015, 1],\n",
       " [2, 60, 8],\n",
       " [2, 3431, 61],\n",
       " [2, 533, 61],\n",
       " [2, 1446, 61],\n",
       " [2, 7129, 61],\n",
       " [2, 175, 61],\n",
       " [2, 7130, 61, 7131, 61],\n",
       " [2, 7131, 61],\n",
       " [2, 6016, 61],\n",
       " [2, 6017, 1],\n",
       " [2, 8788, 61],\n",
       " [2, 32, 61],\n",
       " [2, 2162, 61],\n",
       " [2, 483, 61],\n",
       " [2, 3039, 1],\n",
       " [2, 2163, 1],\n",
       " [2, 3225, 1],\n",
       " [2, 1504, 1],\n",
       " [2, 2164, 794, 61],\n",
       " [2, 8789, 794, 61],\n",
       " [2, 7132, 794, 1],\n",
       " [2, 15, 11933, 1],\n",
       " [2, 15, 5276, 1],\n",
       " [2, 15, 11934, 1],\n",
       " [2, 15, 7133, 1],\n",
       " [2, 2456, 1],\n",
       " [2, 3933, 1],\n",
       " [2, 19, 606, 1],\n",
       " [2, 64, 1582, 61],\n",
       " [2, 2457, 6, 61],\n",
       " [2, 8790, 25, 6018, 1],\n",
       " [2, 533, 61],\n",
       " [2, 7129, 61],\n",
       " [2, 11935, 61],\n",
       " [2, 8791, 61],\n",
       " [2, 8791, 61],\n",
       " [2, 3934, 61],\n",
       " [2, 3226, 1],\n",
       " [2, 36, 2086, 61],\n",
       " [2, 11936, 61],\n",
       " [2, 2086, 61],\n",
       " [2, 11937, 61],\n",
       " [2, 11938, 61],\n",
       " [2, 1222, 1],\n",
       " [2, 302, 78, 155, 1],\n",
       " [2, 3935, 78, 155, 1],\n",
       " [2, 479, 78, 155, 1],\n",
       " [2, 2349, 78, 155, 1],\n",
       " [2, 302, 80, 1],\n",
       " [2, 3935, 80, 1],\n",
       " [2, 479, 80, 1],\n",
       " [2, 2349, 80, 1],\n",
       " [2, 19, 41, 61],\n",
       " [2, 19, 11939, 8],\n",
       " [2, 3648, 8],\n",
       " [2, 5, 1246, 1],\n",
       " [2, 3649, 1247, 1],\n",
       " [2, 8792, 1],\n",
       " [2, 15, 3432, 1],\n",
       " [2, 15, 3433, 1],\n",
       " [2, 7134, 1],\n",
       " [2, 15, 3936, 1],\n",
       " [2, 11940, 1],\n",
       " [2, 15, 11941, 1],\n",
       " [2, 31, 19, 16, 1],\n",
       " [2, 1897, 1],\n",
       " [2, 3040, 1],\n",
       " [2, 577, 1],\n",
       " [2, 8793, 1],\n",
       " [2, 3937, 1],\n",
       " [2, 19, 234, 1],\n",
       " [2, 5277, 1],\n",
       " [2, 4299, 1],\n",
       " [2, 4300, 1],\n",
       " [2, 35, 716, 1],\n",
       " [2, 41, 7135, 1],\n",
       " [2, 35, 1898, 1],\n",
       " [2, 995, 1],\n",
       " [2, 1223, 1],\n",
       " [2, 5278, 1],\n",
       " [2, 6, 70, 86, 61],\n",
       " [2, 10, 484, 522, 1],\n",
       " [2, 10, 484, 522, 61],\n",
       " [2, 1034, 61],\n",
       " [2, 10, 395, 1618, 61],\n",
       " [2, 10, 33, 66, 61],\n",
       " [2, 201, 6019, 61],\n",
       " [2, 8794, 61],\n",
       " [2, 11942, 61],\n",
       " [2, 201, 13, 6020, 61],\n",
       " [2, 13, 784, 8],\n",
       " [2, 11, 100, 8],\n",
       " [2, 260, 61],\n",
       " [2, 260, 1],\n",
       " [2, 8795, 1],\n",
       " [2, 19, 11943, 1],\n",
       " [2, 3650, 1],\n",
       " [2, 18, 4, 31, 8],\n",
       " [2, 1540, 9, 7, 1],\n",
       " [2, 1540, 1],\n",
       " [2, 3431, 61],\n",
       " [2, 1665, 13, 1505, 1],\n",
       " [2, 2837, 1846, 1],\n",
       " [2, 16, 557, 1],\n",
       " [2, 16, 207, 1],\n",
       " [2, 16, 187, 1],\n",
       " [2, 1506, 951, 1],\n",
       " [2, 1506, 870, 1],\n",
       " [2, 264, 187, 1],\n",
       " [2, 264, 207, 1],\n",
       " [2, 2838, 951, 1],\n",
       " [2, 2838, 870, 1],\n",
       " [2, 2838, 11944, 1],\n",
       " [2, 16, 930, 1],\n",
       " [2, 11945, 1],\n",
       " [2, 1291, 1],\n",
       " [2, 8796, 1],\n",
       " [2, 1291, 1],\n",
       " [2, 11946, 1],\n",
       " [2, 541, 1],\n",
       " [2, 635, 1],\n",
       " [2, 6021, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 8797, 1],\n",
       " [2, 3431, 61],\n",
       " [2, 8798, 1],\n",
       " [2, 3938, 9, 7, 1],\n",
       " [2, 8799, 1],\n",
       " [2, 6017, 1],\n",
       " [2, 646, 1],\n",
       " [2, 1897, 1],\n",
       " [2, 4301, 1],\n",
       " [2, 4713, 1],\n",
       " [2, 1052, 10, 42, 61],\n",
       " [2, 4714, 61],\n",
       " [2, 2252, 10, 42, 61],\n",
       " [2, 621, 61],\n",
       " [2, 1052, 80, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 9, 11, 566, 61],\n",
       " [2, 1052, 10, 42, 61],\n",
       " [2, 4714, 61],\n",
       " [2, 621, 61],\n",
       " [2, 1052, 80, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 11947, 1],\n",
       " [2, 3932, 1],\n",
       " [2, 1052, 9, 48, 1],\n",
       " [2, 479, 916, 1],\n",
       " [2, 161, 1199, 61],\n",
       " [2, 161, 11, 693, 1],\n",
       " [2, 11948, 61],\n",
       " [2, 483, 61],\n",
       " [2, 483, 14, 261, 61],\n",
       " [2, 14, 1120, 61],\n",
       " [2, 3938, 4302, 1],\n",
       " [2, 5, 189, 1],\n",
       " [2, 5, 2350, 1],\n",
       " [2, 1541, 1],\n",
       " [2, 1541, 1],\n",
       " [2, 7136, 20, 284, 1],\n",
       " [2, 1541, 1],\n",
       " [2, 6022, 1],\n",
       " [2, 1504, 7, 61],\n",
       " [2, 5279, 9, 7, 1],\n",
       " [2, 483, 1],\n",
       " [2, 8800, 1],\n",
       " [2, 11949, 1],\n",
       " [2, 11950, 1],\n",
       " [2, 11951, 1],\n",
       " [2, 11952, 1],\n",
       " [2, 11953, 1],\n",
       " [2, 11954, 1],\n",
       " [2, 11955, 1],\n",
       " [2, 11956, 1],\n",
       " [2, 8801, 1],\n",
       " [2, 3938, 4302, 1],\n",
       " [2, 8802, 9, 7, 1],\n",
       " [2, 35, 10, 251, 1],\n",
       " [2, 10, 251, 1],\n",
       " [2, 15, 8803, 1],\n",
       " [2, 3934, 1],\n",
       " [2, 15, 11957, 1],\n",
       " [2, 15, 8804, 1],\n",
       " [2, 15, 665, 1847, 1],\n",
       " [2, 15, 64, 11958, 1],\n",
       " [2, 15, 6023, 1],\n",
       " [2, 15, 7137, 1],\n",
       " [2, 15, 64, 8805, 1],\n",
       " [2, 1619, 1],\n",
       " [2, 19, 1174, 1],\n",
       " [2, 705, 1],\n",
       " [2, 79, 7, 1],\n",
       " [2, 35, 1475, 1],\n",
       " [2, 79, 1955, 1],\n",
       " [2, 35, 13, 622, 1],\n",
       " [2, 35, 8806, 1],\n",
       " [2, 79, 410, 1],\n",
       " [2, 79, 1583, 1],\n",
       " [2, 35, 4303, 1],\n",
       " [2, 17, 65, 1],\n",
       " [2, 79, 31, 1],\n",
       " [2, 79, 31, 1],\n",
       " [2, 11959, 9, 145, 1],\n",
       " [2, 16, 342, 281, 1],\n",
       " [2, 7138, 1],\n",
       " [2, 411, 8807, 1],\n",
       " [2, 2351, 5, 5280, 1],\n",
       " [2, 31, 221, 1],\n",
       " [2, 1112, 1],\n",
       " [2, 941, 61],\n",
       " [2, 11960, 1],\n",
       " [2, 11961, 1],\n",
       " [2, 11962, 1],\n",
       " [2, 11963, 1],\n",
       " [2, 11964, 1],\n",
       " [2, 11965, 1],\n",
       " [2, 11966, 1],\n",
       " [2, 11967, 1],\n",
       " [2, 75, 1447, 1],\n",
       " [2, 11968, 1],\n",
       " [2, 8808, 1],\n",
       " [2, 3939, 1],\n",
       " [2, 885, 11, 807, 61],\n",
       " [2, 4304, 5, 3227, 61],\n",
       " [2, 11969, 1],\n",
       " [2, 161, 11, 693, 1],\n",
       " [2, 161, 1199, 1],\n",
       " [2, 7139, 1],\n",
       " [2, 11970, 1],\n",
       " [2, 7140, 1],\n",
       " [2, 4715, 1],\n",
       " [2, 2024, 1],\n",
       " [2, 7, 654, 1],\n",
       " [2, 7, 1246, 1],\n",
       " [2, 7, 942, 1],\n",
       " [2, 5281, 981, 1],\n",
       " [2, 2087, 61],\n",
       " [2, 8809, 61],\n",
       " [2, 11971, 61],\n",
       " [2, 2087, 1],\n",
       " [2, 2691, 30, 468, 1],\n",
       " [2, 75, 7141, 1],\n",
       " [2, 19, 871, 1],\n",
       " [2, 2165, 1],\n",
       " [2, 3940, 1],\n",
       " [2, 6024, 1],\n",
       " [2, 60, 654, 8],\n",
       " [2, 60, 3933, 8],\n",
       " [2, 60, 1246, 8],\n",
       " [2, 60, 942, 8],\n",
       " [2, 60, 54, 1582, 8],\n",
       " [2, 18, 4, 6, 8],\n",
       " [2, 1317, 1],\n",
       " [2, 117, 1582, 1],\n",
       " [2, 35, 1475, 8],\n",
       " [2, 35, 1955, 8],\n",
       " [2, 8810, 1],\n",
       " [2, 8810, 1],\n",
       " [2, 11972, 1],\n",
       " [2, 8811, 61],\n",
       " [2, 8812, 1],\n",
       " [2, 16, 14, 147, 1],\n",
       " [2, 16, 365, 1],\n",
       " [2, 16, 1706, 1],\n",
       " [2, 264, 1706, 1],\n",
       " [2, 2838, 7142, 1],\n",
       " [2, 2837, 1847, 1],\n",
       " [2, 6, 22, 3434, 1],\n",
       " [2, 2839, 9, 73, 61],\n",
       " [2, 2839, 9, 73, 61],\n",
       " [2, 8813, 9, 73, 61],\n",
       " [2, 11973, 1],\n",
       " [2, 636, 1],\n",
       " [2, 4305, 61],\n",
       " [2, 11974, 1],\n",
       " [2, 6, 22, 694, 1],\n",
       " [2, 2163, 1],\n",
       " [2, 11975, 9, 73, 1],\n",
       " [2, 8814, 9, 73, 1],\n",
       " [2, 11976, 9, 73, 1],\n",
       " [2, 11977, 9, 73, 1],\n",
       " [2, 4716, 9, 7, 1],\n",
       " [2, 7143, 9, 7, 1],\n",
       " [2, 6025, 45, 1],\n",
       " [2, 1052, 10, 42, 61],\n",
       " [2, 4714, 61],\n",
       " [2, 621, 61],\n",
       " [2, 1052, 80, 61],\n",
       " [2, 906, 1],\n",
       " [2, 6026, 1],\n",
       " [2, 11978, 61],\n",
       " [2, 1052, 10, 42, 61],\n",
       " [2, 4714, 61],\n",
       " [2, 621, 61],\n",
       " [2, 1052, 80, 61],\n",
       " [2, 11979, 1],\n",
       " [2, 2087, 61],\n",
       " [2, 4306, 65, 61],\n",
       " [2, 1112, 27, 352, 1],\n",
       " [2, 1035, 1],\n",
       " [2, 1035, 1],\n",
       " [2, 541, 1],\n",
       " [2, 1707, 1],\n",
       " [2, 6021, 1],\n",
       " [2, 1707, 1],\n",
       " [2, 11980, 1],\n",
       " [2, 11981, 8],\n",
       " [2, 151, 83, 61],\n",
       " [2, 6027, 9, 7, 1],\n",
       " [2, 11982, 1],\n",
       " [2, 8815, 1],\n",
       " [2, 6028, 65, 1],\n",
       " [2, 6029, 65, 1],\n",
       " [2, 5, 416, 1],\n",
       " [2, 5, 19, 1318, 1],\n",
       " [2, 5, 996, 1],\n",
       " [2, 175, 9, 7, 1],\n",
       " [2, 1504, 4, 62, 8],\n",
       " [2, 8816, 7144, 1],\n",
       " [2, 4, 411, 11983, 8],\n",
       " [2, 951, 1],\n",
       " [2, 4, 380, 257, 106, 8],\n",
       " [2, 1504, 18, 42, 1],\n",
       " [2, 4, 3435, 61],\n",
       " [2, 4, 72, 1795, 8],\n",
       " [2, 37, 10, 1795, 8],\n",
       " [2, 4717, 5, 52, 1],\n",
       " [2, 11984, 1],\n",
       " [2, 2164, 794, 61],\n",
       " [2, 3651, 1],\n",
       " [2, 8789, 794, 1],\n",
       " [2, 11985, 1],\n",
       " [2, 11986, 1],\n",
       " [2, 4307, 1],\n",
       " [2, 8817, 1],\n",
       " [2, 35, 410, 1],\n",
       " [2, 15, 19, 1072, 1],\n",
       " [2, 15, 11, 1072, 1],\n",
       " [2, 46, 69, 1],\n",
       " [2, 11987, 1],\n",
       " [2, 11988, 1],\n",
       " [2, 19, 235, 65, 1],\n",
       " [2, 19, 235, 1],\n",
       " [2, 7145, 1],\n",
       " [2, 19, 738, 1],\n",
       " [2, 15, 30, 3041, 1],\n",
       " [2, 19, 558, 1],\n",
       " [2, 19, 41, 1],\n",
       " [2, 6017, 1],\n",
       " [2, 849, 18, 285, 1],\n",
       " [2, 15, 3652, 1],\n",
       " [2, 8793, 1],\n",
       " [2, 2350, 1],\n",
       " [2, 15, 2166, 1],\n",
       " [2, 19, 197, 1],\n",
       " [2, 11989, 1],\n",
       " [2, 7146, 1],\n",
       " [2, 15, 665, 1],\n",
       " [2, 872, 1],\n",
       " [2, 1956, 1],\n",
       " [2, 8818, 1],\n",
       " [2, 11990, 1],\n",
       " [2, 31, 19, 1542, 1],\n",
       " [2, 982, 1],\n",
       " [2, 31, 2167, 1],\n",
       " [2, 64, 1036, 1],\n",
       " [2, 35, 10, 507, 1],\n",
       " [2, 79, 1957, 1],\n",
       " [2, 35, 1957, 1],\n",
       " [2, 35, 4308, 1],\n",
       " [2, 35, 1846, 1],\n",
       " [2, 64, 567, 1],\n",
       " [2, 35, 1270, 1],\n",
       " [2, 19, 234, 1],\n",
       " [2, 79, 268, 11991, 1],\n",
       " [2, 79, 557, 1],\n",
       " [2, 35, 1755, 1],\n",
       " [2, 79, 461, 61],\n",
       " [2, 31, 79, 461, 1],\n",
       " [2, 35, 647, 1],\n",
       " [2, 35, 907, 1],\n",
       " [2, 80, 15, 4309, 1],\n",
       " [2, 35, 42, 1],\n",
       " [2, 35, 13, 48, 1],\n",
       " [2, 35, 886, 1],\n",
       " [2, 305, 122, 1],\n",
       " [2, 79, 2352, 1],\n",
       " [2, 35, 1584, 1],\n",
       " [2, 79, 547, 1],\n",
       " [2, 15, 850, 9, 21, 1],\n",
       " [2, 35, 1755, 1],\n",
       " [2, 79, 997, 1],\n",
       " [2, 79, 1319, 1],\n",
       " [2, 79, 512, 1],\n",
       " [2, 35, 9, 952, 1],\n",
       " [2, 35, 1148, 1],\n",
       " [2, 79, 3042, 1],\n",
       " [2, 35, 1410, 1],\n",
       " [2, 79, 3941, 1],\n",
       " [2, 35, 7147, 1],\n",
       " [2, 35, 1708, 1],\n",
       " [2, 79, 396, 1],\n",
       " [2, 1796, 1],\n",
       " [2, 33, 175, 1],\n",
       " [2, 655, 1],\n",
       " [2, 717, 1],\n",
       " [2, 12, 7, 1],\n",
       " [2, 12, 572, 1],\n",
       " [2, 12, 1224, 1],\n",
       " [2, 12, 138, 1],\n",
       " [2, 12, 513, 1],\n",
       " [2, 12, 410, 1],\n",
       " [2, 12, 739, 1],\n",
       " [2, 12, 674, 1],\n",
       " [2, 1585, 480, 1],\n",
       " [2, 6, 528, 1],\n",
       " [2, 6030, 9, 73, 1],\n",
       " [2, 6030, 9, 73, 1],\n",
       " [2, 11992, 9, 73, 1],\n",
       " [2, 1899, 1],\n",
       " [2, 391, 1],\n",
       " [2, 3436, 1],\n",
       " [2, 3043, 1],\n",
       " [2, 228, 1],\n",
       " [2, 3431, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 496, 1],\n",
       " [2, 11993, 186, 1],\n",
       " [2, 46, 69, 8],\n",
       " [2, 11994, 9, 7, 1],\n",
       " [2, 23, 189, 1],\n",
       " [2, 23, 266, 1],\n",
       " [2, 23, 1317, 1],\n",
       " [2, 3044, 61],\n",
       " [2, 1958, 1],\n",
       " [2, 3044, 1],\n",
       " [2, 1958, 42, 1],\n",
       " [2, 872, 34, 365, 61],\n",
       " [2, 218, 34, 365, 1],\n",
       " [2, 218, 34, 314, 61],\n",
       " [2, 3653, 1],\n",
       " [2, 14, 261, 1],\n",
       " [2, 3942, 61],\n",
       " [2, 10, 587, 61],\n",
       " [2, 6, 22, 3434, 1],\n",
       " [2, 3943, 9, 7, 1],\n",
       " [2, 4310, 9, 7, 1],\n",
       " [2, 6031, 9, 7, 1],\n",
       " [2, 1448, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 71, 3654, 1],\n",
       " [2, 7, 189, 1],\n",
       " [2, 7, 54, 612, 1],\n",
       " [2, 7, 16, 568, 1],\n",
       " [2, 7, 19, 150, 1],\n",
       " [2, 7, 132, 11995, 10, 599, 1],\n",
       " [2, 7, 16, 49, 1],\n",
       " [2, 7, 1666, 1],\n",
       " [2, 73, 2692, 1],\n",
       " [2, 7, 453, 1],\n",
       " [2, 7, 1091, 1],\n",
       " [2, 7, 2350, 1],\n",
       " [2, 7, 3045, 1],\n",
       " [2, 7, 1271, 1],\n",
       " [2, 7, 16, 54, 1898, 1],\n",
       " [2, 119, 122, 1],\n",
       " [2, 1756, 13, 21, 1],\n",
       " [2, 3228, 10, 1848, 1],\n",
       " [2, 996, 14, 104, 1],\n",
       " [2, 7148, 14, 104, 1],\n",
       " [2, 6032, 14, 104, 1],\n",
       " [2, 6032, 45, 1],\n",
       " [2, 6033, 45, 1],\n",
       " [2, 996, 45, 1],\n",
       " [2, 998, 45, 1],\n",
       " [2, 7149, 9, 7, 1],\n",
       " [2, 11996, 9, 7, 1],\n",
       " [2, 11997, 9, 7, 1],\n",
       " [2, 7149, 9, 7, 1],\n",
       " [2, 3655, 1],\n",
       " [2, 11998, 1],\n",
       " [2, 11999, 1],\n",
       " [2, 12000, 1],\n",
       " [2, 8819, 1],\n",
       " [2, 135, 10, 251, 1],\n",
       " [2, 19, 5282, 1],\n",
       " [2, 1620, 1],\n",
       " [2, 135, 65, 1],\n",
       " [2, 32, 4, 8],\n",
       " [2, 4, 572, 61],\n",
       " [2, 60, 79, 31, 8],\n",
       " [2, 60, 189, 8],\n",
       " [2, 60, 266, 8],\n",
       " [2, 60, 16, 8820, 8],\n",
       " [2, 60, 16, 568, 8],\n",
       " [2, 60, 453, 8],\n",
       " [2, 60, 1091, 8],\n",
       " [2, 60, 16, 54, 417, 8],\n",
       " [2, 60, 19, 54, 917, 8],\n",
       " [2, 60, 3045, 8],\n",
       " [2, 60, 12, 5, 8],\n",
       " [2, 3437, 1],\n",
       " [2, 2840, 1],\n",
       " [2, 117, 422, 1],\n",
       " [2, 12001, 1],\n",
       " [2, 968, 422, 1],\n",
       " [2, 453, 105, 1],\n",
       " [2, 54, 422, 105, 1],\n",
       " [2, 2458, 205, 1],\n",
       " [2, 288, 422, 205, 1],\n",
       " [2, 249, 10, 136, 1],\n",
       " [2, 29, 559, 1],\n",
       " [2, 105, 559, 1],\n",
       " [2, 249, 10, 105, 1],\n",
       " [2, 249, 10, 1014, 1],\n",
       " [2, 3944, 1, 533, 61],\n",
       " [2, 6034, 1],\n",
       " [2, 12002, 1],\n",
       " [2, 12003, 1],\n",
       " [2, 16, 3945, 1],\n",
       " [2, 27, 1149, 3946, 1],\n",
       " [2, 4718, 1],\n",
       " [2, 509, 9, 48, 61],\n",
       " [2, 4305, 1],\n",
       " [2, 8821, 1],\n",
       " [2, 1507, 13, 1073, 1],\n",
       " [2, 8790, 25, 6018, 1],\n",
       " [2, 1292, 14, 683, 1],\n",
       " [2, 224, 69, 8],\n",
       " [2, 224, 1621, 8],\n",
       " [2, 224, 8822, 8],\n",
       " [2, 12004, 9, 73, 61],\n",
       " [2, 12005, 1],\n",
       " [2, 12006, 1],\n",
       " [2, 12007, 1],\n",
       " [2, 12008, 1],\n",
       " [2, 808, 1],\n",
       " [2, 3656, 1],\n",
       " [2, 418, 1],\n",
       " [2, 1959, 42, 1],\n",
       " [2, 418, 9, 48, 1],\n",
       " [2, 1959, 42, 1],\n",
       " [2, 4719, 1],\n",
       " [2, 7150, 1],\n",
       " [2, 636, 42, 1],\n",
       " [2, 808, 204, 1],\n",
       " [2, 31, 1796, 8],\n",
       " [2, 1796, 8],\n",
       " [2, 1136, 78, 1],\n",
       " [2, 27, 523, 12009, 1],\n",
       " [2, 6, 3657, 1],\n",
       " [2, 6, 2088, 1],\n",
       " [2, 6, 2088, 61],\n",
       " [2, 6, 2088, 61],\n",
       " [2, 6, 12010, 1],\n",
       " [2, 6, 12011, 1],\n",
       " [2, 6, 4299, 1],\n",
       " [2, 6, 2088, 1],\n",
       " [2, 6, 22, 3438, 61],\n",
       " [2, 6, 12012, 1],\n",
       " [2, 6, 6035, 1],\n",
       " [2, 3229, 61],\n",
       " [2, 12, 1797, 61],\n",
       " [2, 3431, 61],\n",
       " [2, 12013, 45, 1],\n",
       " [2, 12014, 45, 1],\n",
       " [2, 12015, 45, 1],\n",
       " [2, 12016, 45, 1],\n",
       " [2, 12017, 45, 1],\n",
       " [2, 6036, 1],\n",
       " [2, 12018, 1],\n",
       " [2, 8823, 1],\n",
       " [2, 1899, 1],\n",
       " [2, 12019, 1],\n",
       " [2, 4720, 10, 21, 1],\n",
       " [2, 12020, 1],\n",
       " [2, 12021, 10, 21, 1],\n",
       " [2, 7151, 1],\n",
       " [2, 1507, 9, 599, 1],\n",
       " [2, 302, 9, 18, 599, 1],\n",
       " [2, 302, 9, 4721, 1],\n",
       " [2, 302, 9, 18, 599, 1],\n",
       " [2, 1052, 9, 18, 599, 1],\n",
       " [2, 3935, 9, 18, 599, 1],\n",
       " [2, 1707, 1],\n",
       " [2, 1052, 9, 11, 240, 1],\n",
       " [2, 7140, 1],\n",
       " [2, 3938, 45, 1],\n",
       " [2, 1622, 1709, 1],\n",
       " [2, 732, 55, 1],\n",
       " [2, 824, 1709, 1],\n",
       " [2, 5, 12, 410, 1],\n",
       " [2, 5, 12, 1092, 1],\n",
       " [2, 5, 16, 4311, 1],\n",
       " [2, 1053, 1],\n",
       " [2, 5, 12, 7152, 1],\n",
       " [2, 5, 12, 3230, 1],\n",
       " [2, 5, 12, 207, 1],\n",
       " [2, 5, 12, 512, 1],\n",
       " [2, 12, 512, 1],\n",
       " [2, 429, 35, 1],\n",
       " [2, 3947, 45, 1],\n",
       " [2, 4, 7153, 61],\n",
       " [2, 4, 1074, 61],\n",
       " [2, 37, 17, 7, 8],\n",
       " [2, 7154, 9, 7, 1],\n",
       " [2, 35, 647, 1],\n",
       " [2, 35, 907, 1],\n",
       " [2, 79, 207, 1],\n",
       " [2, 35, 1148, 1],\n",
       " [2, 35, 1900, 1],\n",
       " [2, 22, 19, 4722, 1],\n",
       " [2, 22, 19, 4723, 1],\n",
       " [2, 12022, 1],\n",
       " [2, 46, 1543, 1],\n",
       " [2, 46, 753, 1],\n",
       " [2, 16, 753, 1],\n",
       " [2, 16, 1544, 1],\n",
       " [2, 887, 1960, 1],\n",
       " [2, 12023, 1],\n",
       " [2, 577, 11, 5283, 1],\n",
       " [2, 665, 2693, 1],\n",
       " [2, 19, 588, 1],\n",
       " [2, 15, 8824, 1],\n",
       " [2, 5284, 11, 1757, 1],\n",
       " [2, 5285, 1],\n",
       " [2, 22, 558, 1],\n",
       " [2, 22, 3231, 1],\n",
       " [2, 80, 22, 3231, 1],\n",
       " [2, 19, 234, 1],\n",
       " [2, 15, 1710, 1],\n",
       " [2, 15, 7155, 1],\n",
       " [2, 7146, 1],\n",
       " [2, 19, 445, 1],\n",
       " [2, 15, 11, 2567, 1],\n",
       " [2, 15, 11, 1292, 1],\n",
       " [2, 6037, 9, 7, 1],\n",
       " [2, 1137, 9, 7, 1],\n",
       " [2, 33, 206, 1],\n",
       " [2, 15, 8825, 1],\n",
       " [2, 15, 12024, 1],\n",
       " [2, 19, 150, 1],\n",
       " [2, 15, 4724, 1],\n",
       " [2, 15, 51, 1],\n",
       " [2, 19, 1961, 61],\n",
       " [2, 15, 398, 1],\n",
       " [2, 416, 13, 784, 1],\n",
       " [2, 19, 1150, 13, 784, 1],\n",
       " [2, 416, 13, 784, 1],\n",
       " [2, 19, 830, 13, 569, 1],\n",
       " [2, 19, 830, 10, 241, 1],\n",
       " [2, 19, 8826, 1],\n",
       " [2, 19, 121, 1],\n",
       " [2, 197, 9, 7, 1],\n",
       " [2, 24, 197, 1],\n",
       " [2, 19, 197, 1],\n",
       " [2, 197, 202, 1],\n",
       " [2, 197, 9, 202, 1],\n",
       " [2, 1200, 9, 202, 1],\n",
       " [2, 22, 197, 1],\n",
       " [2, 22, 64, 195, 1],\n",
       " [2, 350, 9, 7, 1],\n",
       " [2, 15, 8827, 1],\n",
       " [2, 640, 674, 1],\n",
       " [2, 31, 8828, 1],\n",
       " [2, 31, 12025, 1],\n",
       " [2, 7156, 1],\n",
       " [2, 12026, 1],\n",
       " [2, 12027, 1],\n",
       " [2, 19, 1449, 1],\n",
       " [2, 2089, 1],\n",
       " [2, 12028, 1],\n",
       " [2, 12029, 1],\n",
       " [2, 12030, 1],\n",
       " [2, 91, 9, 262, 1],\n",
       " [2, 79, 14, 147, 1],\n",
       " [2, 79, 14, 12031, 1],\n",
       " [2, 35, 63, 1],\n",
       " [2, 35, 366, 1],\n",
       " [2, 31, 35, 63, 1],\n",
       " [2, 31, 35, 366, 1],\n",
       " [2, 35, 943, 1],\n",
       " [2, 35, 2168, 1],\n",
       " [2, 35, 981, 1],\n",
       " [2, 79, 2841, 1],\n",
       " [2, 35, 178, 96, 1],\n",
       " [2, 35, 4725, 1],\n",
       " [2, 35, 178, 1623, 1],\n",
       " [2, 6, 41, 1623, 1],\n",
       " [2, 35, 542, 1],\n",
       " [2, 35, 944, 1],\n",
       " [2, 35, 8829, 1],\n",
       " [2, 35, 8830, 1],\n",
       " [2, 15, 35, 2353, 1],\n",
       " [2, 79, 5, 559, 1],\n",
       " [2, 91, 5, 559, 1],\n",
       " [2, 15, 850, 559, 1],\n",
       " [2, 31, 91, 559, 1],\n",
       " [2, 79, 154, 1],\n",
       " [2, 35, 154, 1],\n",
       " [2, 79, 5286, 1],\n",
       " [2, 79, 8831, 1],\n",
       " [2, 79, 8832, 1],\n",
       " [2, 35, 396, 61],\n",
       " [2, 35, 396, 1],\n",
       " [2, 19, 222, 1],\n",
       " [2, 31, 35, 325, 1],\n",
       " [2, 31, 35, 1054, 1],\n",
       " [2, 79, 795, 1],\n",
       " [2, 79, 1075, 1],\n",
       " [2, 79, 1224, 1],\n",
       " [2, 64, 422, 1],\n",
       " [2, 7157, 1],\n",
       " [2, 6, 530, 440, 1],\n",
       " [2, 17, 65, 7, 8],\n",
       " [2, 17, 7, 8],\n",
       " [2, 12, 2090, 8],\n",
       " [2, 12, 547, 8],\n",
       " [2, 17, 471, 8],\n",
       " [2, 101, 29, 8],\n",
       " [2, 16, 2842, 1],\n",
       " [2, 6, 19, 1320, 1],\n",
       " [2, 2568, 1],\n",
       " [2, 1586, 1],\n",
       " [2, 12032, 1],\n",
       " [2, 1508, 1],\n",
       " [2, 3046, 1],\n",
       " [2, 3948, 1],\n",
       " [2, 2459, 1],\n",
       " [2, 56, 30, 1],\n",
       " [2, 12, 14, 1509, 1],\n",
       " [2, 108, 329, 1],\n",
       " [2, 17, 329, 1],\n",
       " [2, 17, 329, 1],\n",
       " [2, 17, 1476, 1],\n",
       " [2, 17, 1321, 1],\n",
       " [2, 17, 396, 61],\n",
       " [2, 17, 941, 1],\n",
       " [2, 17, 65, 1],\n",
       " [2, 12, 207, 1],\n",
       " [2, 17, 42, 1],\n",
       " [2, 12, 10, 23, 1],\n",
       " [2, 12, 122, 1],\n",
       " [2, 12, 441, 1],\n",
       " [2, 12, 607, 1],\n",
       " [2, 12, 930, 1],\n",
       " [2, 108, 151, 77, 1],\n",
       " [2, 613, 1],\n",
       " [2, 17, 65, 1],\n",
       " [2, 12, 237, 1],\n",
       " [2, 12, 11, 281, 1],\n",
       " [2, 16, 718, 1],\n",
       " [2, 16, 402, 1],\n",
       " [2, 54, 999, 5, 261, 1],\n",
       " [2, 12, 11, 133, 1],\n",
       " [2, 12, 5, 261, 1],\n",
       " [2, 54, 999, 11, 133, 1],\n",
       " [2, 12, 100, 61],\n",
       " [2, 12, 100, 1],\n",
       " [2, 12, 21, 83, 1],\n",
       " [2, 12033, 1],\n",
       " [2, 12034, 1],\n",
       " [2, 12035, 1],\n",
       " [2, 2694, 45, 1],\n",
       " [2, 4726, 45, 1],\n",
       " [2, 2694, 45, 1],\n",
       " [2, 12036, 45, 1],\n",
       " [2, 5287, 45, 1],\n",
       " [2, 1665, 825, 1],\n",
       " [2, 4312, 825, 1],\n",
       " [2, 7158, 825, 1],\n",
       " [2, 7158, 7159, 1],\n",
       " [2, 1665, 7159, 1],\n",
       " [2, 4312, 7159, 1],\n",
       " [2, 7160, 6038, 1],\n",
       " [2, 7160, 12037, 1],\n",
       " [2, 1899, 9, 73, 1],\n",
       " [2, 1899, 9, 73, 1],\n",
       " [2, 5288, 9, 73, 1],\n",
       " [2, 78, 8833, 1],\n",
       " [2, 78, 1052, 1],\n",
       " [2, 78, 4714, 1],\n",
       " [2, 391, 6039, 61],\n",
       " [2, 5289, 61],\n",
       " [2, 3047, 69, 61],\n",
       " [2, 3047, 69, 61],\n",
       " [2, 391, 528, 1],\n",
       " [2, 3047, 528, 1],\n",
       " [2, 3436, 528, 1],\n",
       " [2, 2695, 1],\n",
       " [2, 1711, 1],\n",
       " [2, 7161, 61],\n",
       " [2, 6026, 13, 1037, 1],\n",
       " [2, 1665, 1847, 13, 5, 719, 1],\n",
       " [2, 995, 1],\n",
       " [2, 462, 32, 239, 381, 1],\n",
       " [2, 462, 32, 239, 381, 1],\n",
       " [2, 1293, 32, 239, 381, 1],\n",
       " [2, 3658, 32, 239, 381, 1],\n",
       " [2, 462, 293, 873, 61],\n",
       " [2, 462, 873, 1],\n",
       " [2, 1293, 873, 1],\n",
       " [2, 3659, 873, 1],\n",
       " [2, 462, 42, 1],\n",
       " [2, 3659, 42, 1],\n",
       " [2, 3658, 42, 1],\n",
       " [2, 12038, 1],\n",
       " [2, 8798, 1],\n",
       " [2, 12039, 1],\n",
       " [2, 12040, 1],\n",
       " [2, 4727, 9, 14, 381, 1],\n",
       " [2, 319, 740, 1],\n",
       " [2, 3439, 5, 8834, 1],\n",
       " [2, 319, 480, 1],\n",
       " [2, 151, 1510, 61],\n",
       " [2, 18, 2169, 61],\n",
       " [2, 7129, 61],\n",
       " [2, 533, 9, 12041, 61],\n",
       " [2, 3440, 8],\n",
       " [2, 18, 52, 302, 1],\n",
       " [2, 18, 52, 1052, 1],\n",
       " [2, 18, 52, 8833, 1],\n",
       " [2, 18, 52, 3932, 1],\n",
       " [2, 12042, 1],\n",
       " [2, 12043, 1],\n",
       " [2, 12044, 1],\n",
       " [2, 12045, 1],\n",
       " [2, 1121, 45, 1],\n",
       " [2, 2354, 45, 1],\n",
       " [2, 7162, 45, 1],\n",
       " [2, 8835, 45, 1],\n",
       " [2, 4728, 1504, 1],\n",
       " [2, 9, 21, 4, 15, 12046, 1],\n",
       " [2, 8836, 1272, 1],\n",
       " [2, 8836, 34, 1272, 1],\n",
       " [2, 9, 18, 5, 61],\n",
       " [2, 12047, 61],\n",
       " [2, 12048, 61],\n",
       " [2, 12049, 61],\n",
       " [2, 12050, 61],\n",
       " [2, 13, 784, 8],\n",
       " [2, 23, 19, 606, 1],\n",
       " [2, 23, 19, 4313, 1],\n",
       " [2, 23, 969, 1],\n",
       " [2, 23, 2170, 1],\n",
       " [2, 969, 1],\n",
       " [2, 23, 12, 12051, 1],\n",
       " [2, 17, 37, 14, 245, 1],\n",
       " [2, 17, 10, 732, 706, 44, 12052, 1],\n",
       " [2, 17, 187, 1],\n",
       " [2, 17, 4, 12053, 1],\n",
       " [2, 2355, 42, 1],\n",
       " [2, 2696, 45, 1],\n",
       " [2, 2355, 45, 1],\n",
       " [2, 8837, 45, 1],\n",
       " [2, 2696, 45, 1],\n",
       " [2, 8838, 45, 1],\n",
       " [2, 1958, 44, 6, 22, 3434, 1],\n",
       " [2, 1958, 44, 1076, 1847, 1],\n",
       " [2, 3044, 170, 1],\n",
       " [2, 29, 1665, 1],\n",
       " [2, 1847, 1],\n",
       " [2, 34, 916, 1],\n",
       " [2, 198, 1],\n",
       " [2, 4314, 1],\n",
       " [2, 2171, 11, 1505, 1],\n",
       " [2, 6040, 11, 1505, 1],\n",
       " [2, 1076, 429, 1],\n",
       " [2, 1076, 13, 48, 1],\n",
       " [2, 1665, 3042, 1],\n",
       " [2, 432, 14, 196, 873, 1],\n",
       " [2, 32, 42, 1],\n",
       " [2, 3942, 42, 1],\n",
       " [2, 3949, 42, 1],\n",
       " [2, 12054, 42, 1],\n",
       " [2, 12055, 42, 1],\n",
       " [2, 12056, 42, 1],\n",
       " [2, 8788, 42, 1],\n",
       " [2, 32, 33, 1],\n",
       " [2, 12057, 1],\n",
       " [2, 12058, 1],\n",
       " [2, 4315, 61],\n",
       " [2, 22, 12059, 61],\n",
       " [2, 4315, 1],\n",
       " [2, 8839, 1],\n",
       " [2, 1622, 5, 441, 1],\n",
       " [2, 732, 5, 441, 1],\n",
       " [2, 12060, 5, 1201, 1],\n",
       " [2, 732, 5, 1624, 1],\n",
       " [2, 7163, 29, 1],\n",
       " [2, 732, 45, 1],\n",
       " [2, 824, 45, 1],\n",
       " [2, 5290, 45, 1],\n",
       " [2, 732, 45, 1],\n",
       " [2, 260, 61],\n",
       " [2, 260, 9, 136, 1],\n",
       " [2, 260, 1],\n",
       " [2, 33, 12, 1],\n",
       " [2, 79, 31, 1],\n",
       " [2, 89, 79, 31, 1],\n",
       " [2, 589, 4, 8],\n",
       " [2, 3660, 1],\n",
       " [2, 16, 454, 1],\n",
       " [2, 16, 7164, 1],\n",
       " [2, 16, 3232, 1],\n",
       " [2, 2458, 1],\n",
       " [2, 7, 16, 2356, 1],\n",
       " [2, 7, 16, 3432, 1],\n",
       " [2, 7, 16, 7165, 1],\n",
       " [2, 9, 7, 24, 4729, 1],\n",
       " [2, 7, 16, 1798, 1],\n",
       " [2, 9, 7, 24, 392, 1],\n",
       " [2, 7, 1271, 1],\n",
       " [2, 7, 12061, 1],\n",
       " [2, 73, 2460, 1],\n",
       " [2, 7, 17, 65, 1],\n",
       " [2, 7, 17, 626, 1],\n",
       " [2, 7, 17, 1898, 1],\n",
       " [2, 7, 12062, 1],\n",
       " [2, 7, 110, 1],\n",
       " [2, 7, 16, 2357, 1],\n",
       " [2, 7, 16, 3441, 1],\n",
       " [2, 7, 16, 12063, 1],\n",
       " [2, 7, 16, 433, 1],\n",
       " [2, 7, 66, 1],\n",
       " [2, 7, 3442, 1],\n",
       " [2, 73, 19, 606, 1],\n",
       " [2, 73, 19, 1318, 1],\n",
       " [2, 73, 3048, 1],\n",
       " [2, 7, 2170, 1],\n",
       " [2, 7, 2843, 1],\n",
       " [2, 73, 534, 1],\n",
       " [2, 7, 1450, 1],\n",
       " [2, 7, 17, 1475, 1],\n",
       " [2, 7, 17, 542, 1],\n",
       " [2, 7, 17, 674, 1],\n",
       " [2, 7, 12, 1583, 1],\n",
       " [2, 1756, 13, 7, 1],\n",
       " [2, 3228, 10, 138, 1],\n",
       " [2, 12064, 173, 87, 1],\n",
       " [2, 1318, 173, 87, 1],\n",
       " [2, 8840, 1],\n",
       " [2, 1112, 5, 5280, 1],\n",
       " [2, 483, 42, 1],\n",
       " [2, 4730, 42, 1],\n",
       " [2, 3039, 42, 1],\n",
       " [2, 3233, 42, 1],\n",
       " [2, 3950, 9, 7, 1],\n",
       " [2, 8841, 61],\n",
       " [2, 224, 69, 1],\n",
       " [2, 224, 1621, 1],\n",
       " [2, 75, 224, 6039, 1],\n",
       " [2, 19, 7166, 1],\n",
       " [2, 12065, 1],\n",
       " [2, 332, 4316, 1],\n",
       " [2, 75, 5291, 1],\n",
       " [2, 19, 970, 1],\n",
       " [2, 19, 5292, 1],\n",
       " [2, 5293, 1],\n",
       " [2, 145, 1799, 1],\n",
       " [2, 3443, 1],\n",
       " [2, 249, 3443, 1],\n",
       " [2, 80, 3443, 1],\n",
       " [2, 19, 8842, 1],\n",
       " [2, 332, 1582, 61],\n",
       " [2, 65, 149, 1],\n",
       " [2, 4, 62, 8],\n",
       " [2, 9, 60, 24, 392, 8],\n",
       " [2, 60, 2460, 8],\n",
       " [2, 60, 5294, 8],\n",
       " [2, 60, 12, 5, 8],\n",
       " [2, 60, 12, 8],\n",
       " [2, 12066, 1, 1, 1],\n",
       " [2, 60, 16, 433, 8],\n",
       " [2, 60, 1450, 8],\n",
       " [2, 60, 12, 7, 8],\n",
       " [2, 1015, 9, 7, 1],\n",
       " [2, 29, 2569, 1],\n",
       " [2, 3444, 10, 1000, 61],\n",
       " [2, 29, 931, 1],\n",
       " [2, 19, 7167, 1],\n",
       " [2, 29, 58, 1755, 1],\n",
       " [2, 58, 1755, 1],\n",
       " [2, 34, 314, 1],\n",
       " [2, 12067, 34, 314, 1],\n",
       " [2, 59, 9, 3445, 61],\n",
       " [2, 41, 303, 8],\n",
       " [2, 15, 4731, 8],\n",
       " [2, 35, 1667, 8],\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 10459\n",
      "스페인어 단어 집합의 크기 : 20329\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_spa.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}\".format(src_vocab_size))\n",
    "print(\"스페인어 단어 집합의 크기 : {:d}\".format(tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
    "\n",
    "tar_to_index = tokenizer_spa.word_index # 훈련 후 예측 과정에서 사용\n",
    "index_to_tar = tokenizer_spa.index_word # 훈련 후 결과 비교할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82852 71829 84764 ... 98840 99364 92023]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0], dtype=int)\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82852, 71829, 84764, 71945, 74269, 86806, 95184, 77899, 84453,\n",
       "       38711, 13851, 68985, 18542, 28642, 45289, 70341, 72820, 31150,\n",
       "        8251, 54348, 80127, 37055, 57989, 59105, 11803, 16109, 13177,\n",
       "       63576, 59684, 65787,  7840, 86191, 82985, 40740, 98352, 67672,\n",
       "         281, 81158, 96823, 51658, 99892, 58869, 95391,   894, 82876,\n",
       "       33407, 80012,  9812, 52913, 36413, 19973, 99930, 51802, 84299,\n",
       "       53794, 14205, 43079, 89524,  5394, 52738, 49281, 73449, 61540,\n",
       "        7554, 48019, 79861, 18683, 92977, 44494, 22546,  1205, 46498,\n",
       "       67947, 36415, 62476, 67144, 20151, 71500, 61509, 23662, 83945,\n",
       "       15803,  8740, 22371, 99246, 17879,  8776, 61020, 52376,  1856,\n",
       "        5935, 90736,  5488, 61356, 52584, 45108, 32925, 10927, 39682,\n",
       "       30290])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(100000*0.1)\n",
    "print(n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 13)\n",
      "(90000, 17)\n",
      "(90000, 17)\n",
      "(10000, 13)\n",
      "(10000, 17)\n",
      "(10000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     522950      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     1016450     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 50)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50), (None,  20200       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 20329)  1036779     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,616,579\n",
      "Trainable params: 2,616,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node broadcast_weights_1/assert_broadcastable/AssertGuard/else/_13/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_2/_80}}]]\n\t [[broadcast_weights_1/assert_broadcastable/is_valid_shape/else/_1/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_53/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/_64]] [Op:__inference_train_function_19100]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-bc6683695d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n\u001b[0;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_target_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m           batch_size = 128, epochs = 50)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node broadcast_weights_1/assert_broadcastable/AssertGuard/else/_13/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_2/_80}}]]\n\t [[broadcast_weights_1/assert_broadcastable/is_valid_shape/else/_1/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_53/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/_64]] [Op:__inference_train_function_19100]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_src[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            temp = temp + index_to_tar[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node cond/then/_0/cond/CudnnRNNV3}}]]\n\t [[functional_5/lstm_2/PartitionedCall]] [Op:__inference_predict_function_21383]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-42e72b3110aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"원문 : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq2src\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-8b470a37d19f>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 입력으로부터 인코더의 상태를 얻음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstates_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# <SOS>에 해당하는 정수 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    844\u001b[0m               *args, **kwds)\n\u001b[0;32m    845\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node cond/then/_0/cond/CudnnRNNV3}}]]\n\t [[functional_5/lstm_2/PartitionedCall]] [Op:__inference_predict_function_21383]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_test[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_test[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
